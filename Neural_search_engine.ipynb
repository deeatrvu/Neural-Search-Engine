{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-20T16:04:01.699230Z",
     "iopub.status.busy": "2025-05-20T16:04:01.698947Z",
     "iopub.status.idle": "2025-05-20T16:04:01.706608Z",
     "shell.execute_reply": "2025-05-20T16:04:01.704956Z",
     "shell.execute_reply.started": "2025-05-20T16:04:01.699209Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "\n",
    "file_path = \"/kaggle/input/arxiv\"\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/arxiv'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLEANING, DROPPING UNNECESSARY ROWS AND COLUMNS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:04:01.713882Z",
     "iopub.status.busy": "2025-05-20T16:04:01.713529Z",
     "iopub.status.idle": "2025-05-20T16:05:33.246342Z",
     "shell.execute_reply": "2025-05-20T16:05:33.245529Z",
     "shell.execute_reply.started": "2025-05-20T16:04:01.713856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking nulls: 2735264it [01:31, 29900.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total records: 2735264\n",
      "\n",
      "Missing value summary (null or empty):\n",
      "\n",
      "id             :      0 missing (0.00%)\n",
      "submitter      :  15189 missing (0.56%)\n",
      "authors        :      0 missing (0.00%)\n",
      "title          :      0 missing (0.00%)\n",
      "comments       : 713227 missing (26.08%)\n",
      "journal-ref    : 1842970 missing (67.38%)\n",
      "doi            : 1494843 missing (54.65%)\n",
      "report-no      : 2550013 missing (93.23%)\n",
      "categories     :      0 missing (0.00%)\n",
      "license        : 452782 missing (16.55%)\n",
      "abstract       :      0 missing (0.00%)\n",
      "update_date    :      0 missing (0.00%)\n",
      "versions       :      0 missing (0.00%)\n",
      "authors_parsed :      0 missing (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------- Config ---------\n",
    "input_json_path = \"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "# --------- Columns to Analyze ---------\n",
    "target_columns = [\n",
    "    \"id\", \"submitter\", \"authors\", \"title\", \"comments\",\n",
    "    \"journal-ref\", \"doi\", \"report-no\", \"categories\",\n",
    "    \"license\", \"abstract\", \"update_date\", \"versions\", \"authors_parsed\"\n",
    "]\n",
    "\n",
    "# --------- Initialize Counters ---------\n",
    "null_counts = defaultdict(int)\n",
    "total_count = 0\n",
    "\n",
    "# --------- Line-by-Line Parsing ---------\n",
    "with open(input_json_path, \"r\") as f:\n",
    "    for line in tqdm(f, desc=\"Checking nulls\"):\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "            total_count += 1\n",
    "            for col in target_columns:\n",
    "                value = paper.get(col, None)\n",
    "                if value is None or (isinstance(value, str) and value.strip() == \"\"):\n",
    "                    null_counts[col] += 1\n",
    "        except json.JSONDecodeError:\n",
    "            continue  # skip malformed lines\n",
    "\n",
    "# --------- Output Report ---------\n",
    "print(f\"\\nTotal records: {total_count}\\n\")\n",
    "print(\"Missing value summary (null or empty):\\n\")\n",
    "for col in target_columns:\n",
    "    missing = null_counts[col]\n",
    "    percent = (missing / total_count) * 100\n",
    "    print(f\"{col:15}: {missing:6} missing ({percent:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DROPPING FILES WITHOUT DOI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:05:33.247927Z",
     "iopub.status.busy": "2025-05-20T16:05:33.247660Z",
     "iopub.status.idle": "2025-05-20T16:07:54.044253Z",
     "shell.execute_reply": "2025-05-20T16:07:54.042926Z",
     "shell.execute_reply.started": "2025-05-20T16:05:33.247906Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering papers with DOI: 2735264it [02:20, 19428.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total papers processed: 2735264\n",
      "üìå Papers with DOI retained: 1240421\n",
      "üìù Filtered output saved to: /kaggle/working/papers_with_doi_only.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_json_path = \"/kaggle/working/papers_with_doi_only.jsonl\"\n",
    "# --------- Filter and Write ---------\n",
    "total = 0\n",
    "kept = 0\n",
    "\n",
    "with open(input_json_path, \"r\") as infile, open(output_json_path, \"w\") as outfile:\n",
    "    for line in tqdm(infile, desc=\"Filtering papers with DOI\"):\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "            total += 1\n",
    "            if paper.get(\"doi\"):\n",
    "                json.dump(paper, outfile)\n",
    "                outfile.write(\"\\n\")\n",
    "                kept += 1\n",
    "        except json.JSONDecodeError:\n",
    "            continue  # skip bad lines\n",
    "\n",
    "# --------- Summary ---------\n",
    "print(f\"\\n‚úÖ Total papers processed: {total}\")\n",
    "print(f\"üìå Papers with DOI retained: {kept}\")\n",
    "print(f\"üìù Filtered output saved to: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONVERTING TO URL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:07:54.046006Z",
     "iopub.status.busy": "2025-05-20T16:07:54.045683Z",
     "iopub.status.idle": "2025-05-20T16:09:30.428978Z",
     "shell.execute_reply": "2025-05-20T16:09:30.427892Z",
     "shell.execute_reply.started": "2025-05-20T16:07:54.045984Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting DOI to URLs: 1240421it [01:36, 12871.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done! Converted DOI to URLs.\n",
      "üìÅ Output saved to: /kaggle/working/papers_with_urls.jsonl\n",
      "üìä Total papers with URLs: 1240421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------- Paths ---------\n",
    "input_path = \"/kaggle/working/papers_with_doi_only.jsonl\"\n",
    "output_path = \"/kaggle/working/papers_with_urls.jsonl\"\n",
    "\n",
    "# --------- Counter for Papers with URL ---------\n",
    "papers_with_url_count = 0\n",
    "\n",
    "# --------- Transform and Save ---------\n",
    "with open(input_path, \"r\") as infile, open(output_path, \"w\") as outfile:\n",
    "    for line in tqdm(infile, desc=\"Converting DOI to URLs\"):\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "            doi_value = paper.pop(\"doi\", None)\n",
    "            if doi_value:\n",
    "                paper[\"url\"] = f\"https://doi.org/{doi_value}\"\n",
    "                papers_with_url_count += 1  # Increment the counter when URL is added\n",
    "                json.dump(paper, outfile)\n",
    "                outfile.write(\"\\n\")\n",
    "        except json.JSONDecodeError:\n",
    "            continue  # skip malformed lines\n",
    "\n",
    "# --------- Summary ---------\n",
    "print(f\"\\n‚úÖ Done! Converted DOI to URLs.\")\n",
    "print(f\"üìÅ Output saved to: {output_path}\")\n",
    "print(f\"üìä Total papers with URLs: {papers_with_url_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true
   },
   "source": [
    "**DELETIONS IF NEEDED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-05-20T16:09:30.431666Z",
     "iopub.status.busy": "2025-05-20T16:09:30.431290Z",
     "iopub.status.idle": "2025-05-20T16:09:30.436039Z",
     "shell.execute_reply": "2025-05-20T16:09:30.434835Z",
     "shell.execute_reply.started": "2025-05-20T16:09:30.431642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Specify the path of the file you want to delete\n",
    "# file_path = \"\"\n",
    "\n",
    "# # Check if the file exists before deleting it\n",
    "# if os.path.exists(file_path):\n",
    "#     os.remove(file_path)\n",
    "#     print(f\"File {file_path} has been deleted.\")\n",
    "# else:\n",
    "#     print(f\"File {file_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-05-20T16:09:30.437916Z",
     "iopub.status.busy": "2025-05-20T16:09:30.437580Z",
     "iopub.status.idle": "2025-05-20T16:09:30.465779Z",
     "shell.execute_reply": "2025-05-20T16:09:30.464774Z",
     "shell.execute_reply.started": "2025-05-20T16:09:30.437888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Specify the path of the directory you want to delete\n",
    "# dir_path = \"\"\n",
    "\n",
    "# # Check if the directory exists before deleting it\n",
    "# if os.path.exists(dir_path) and os.path.isdir(dir_path):\n",
    "#     shutil.rmtree(dir_path)\n",
    "#     print(f\"Directory {dir_path} has been deleted.\")\n",
    "# else:\n",
    "#     print(f\"Directory {dir_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**EXTRACT EQUATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:09:30.467447Z",
     "iopub.status.busy": "2025-05-20T16:09:30.467083Z",
     "iopub.status.idle": "2025-05-20T16:10:44.821223Z",
     "shell.execute_reply": "2025-05-20T16:10:44.820389Z",
     "shell.execute_reply.started": "2025-05-20T16:09:30.467391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted equations saved to '/kaggle/working/extracted_equations.jsonl'. Total papers: 421249\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Load research papers from JSONL file\n",
    "file_path = \"/kaggle/working/papers_with_urls.jsonl\"\n",
    "\n",
    "data = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "# Function to extract LaTeX equations\n",
    "def extract_equations(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    inline_eqs = re.findall(r'\\$([^\\$]+)\\$', text)\n",
    "    display_eqs = re.findall(r'\\\\\\[(.*?)\\\\\\]', text)\n",
    "    commands = re.findall(r'\\\\[a-zA-Z]+(?:\\{.*?\\})*', text)\n",
    "    begin_envs = re.findall(r'\\\\begin\\{.*?\\}(.*?)\\\\end\\{.*?\\}', text, re.DOTALL)\n",
    "\n",
    "    all_eqs = inline_eqs + display_eqs + commands + begin_envs\n",
    "\n",
    "    # Filter out too short or meaningless ones\n",
    "    filtered_eqs = []\n",
    "    for eq in all_eqs:\n",
    "        eq = eq.strip()\n",
    "        if len(eq) >= 2 and (any(c in eq for c in \"\\\\{}_^\")):\n",
    "            filtered_eqs.append(eq)\n",
    "\n",
    "    return list(set(filtered_eqs))\n",
    "\n",
    "\n",
    "# Extract equations and store results\n",
    "papers = []\n",
    "for paper in data:\n",
    "    abstract = paper.get(\"abstract\", \"\")\n",
    "    title = paper.get(\"title\", \"\")\n",
    "    url = paper.get(\"url\", \"\") \n",
    "    equations = extract_equations(abstract + \" \" + title)\n",
    "    \n",
    "    if equations:\n",
    "        papers.append({\n",
    "            \"id\": paper.get(\"id\", \"\"),\n",
    "            \"title\": title.replace(\"\\n\", \" \").strip(),\n",
    "            \"abstract\": abstract.replace(\"\\n\", \" \").strip(),\n",
    "            \"equations\": equations,\n",
    "            \"url\": url  \n",
    "\n",
    "        })\n",
    "\n",
    "# Save as JSONL\n",
    "output_path = \"/kaggle/working/extracted_equations.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for paper in papers:\n",
    "        json.dump(paper, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Extracted equations saved to '{output_path}'. Total papers: {len(papers)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO CONVERT TO ZIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:10:44.822575Z",
     "iopub.status.busy": "2025-05-20T16:10:44.822272Z",
     "iopub.status.idle": "2025-05-20T16:10:44.827773Z",
     "shell.execute_reply": "2025-05-20T16:10:44.826750Z",
     "shell.execute_reply.started": "2025-05-20T16:10:44.822555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Zip the papers_with_urls.jsonl file\n",
    "# shutil.make_archive(\n",
    "#     base_name=\"/kaggle/working/extracted_equations\",  # Output path (without .zip)\n",
    "#     format='zip',\n",
    "#     root_dir=\"/kaggle/working\",                   # Directory containing the file\n",
    "#     base_dir=\"/kaggle/working/extracted_equations.jsonl\"             # File to zip\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:10:44.829533Z",
     "iopub.status.busy": "2025-05-20T16:10:44.829077Z",
     "iopub.status.idle": "2025-05-20T16:10:50.736936Z",
     "shell.execute_reply": "2025-05-20T16:10:50.735754Z",
     "shell.execute_reply.started": "2025-05-20T16:10:44.829501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:10:50.739233Z",
     "iopub.status.busy": "2025-05-20T16:10:50.738798Z",
     "iopub.status.idle": "2025-05-20T16:10:50.747079Z",
     "shell.execute_reply": "2025-05-20T16:10:50.745820Z",
     "shell.execute_reply.started": "2025-05-20T16:10:50.739193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import gc\n",
    "# import faiss\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# # --- CONFIG ---\n",
    "# input_path = \"/kaggle/working/extracted_equations.jsonl\"\n",
    "# output_text_faiss = \"/kaggle/working/text_faiss_index.bin\"\n",
    "# output_eq_faiss = \"/kaggle/working/eq_faiss_index.bin\"\n",
    "# output_text_mapping = \"/kaggle/working/text_id_title_mapping.csv\"\n",
    "# output_eq_mapping = \"/kaggle/working/eq_id_eq_mapping.csv\"\n",
    "\n",
    "# batch_size = 5000  # Good for Kaggle RAM\n",
    "\n",
    "# # --- Load SciBERT ---\n",
    "# model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # --- Helper ---\n",
    "# def generate_embedding(text):\n",
    "#     if not text.strip():\n",
    "#         return None\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# # --- Initialize FAISS ---\n",
    "# embedding_dim = 768\n",
    "# text_index = faiss.IndexFlatL2(embedding_dim)\n",
    "# eq_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# # --- Mappings ---\n",
    "# text_paper_ids = []\n",
    "# text_titles = []\n",
    "\n",
    "# eq_paper_ids = []\n",
    "# eq_strings = []\n",
    "\n",
    "# # --- Buffers ---\n",
    "# text_embeddings_buffer = []\n",
    "# eq_embeddings_buffer = []\n",
    "\n",
    "# # --- Start ---\n",
    "# with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "#     for idx, line in tqdm(enumerate(infile), desc=\"Processing Papers\"):\n",
    "#         try:\n",
    "#             paper = json.loads(line)\n",
    "#             paper_id = paper.get(\"id\", \"\")\n",
    "#             title = paper.get(\"title\", \"\")\n",
    "#             abstract = paper.get(\"abstract\", \"\")\n",
    "#             equations = paper.get(\"equations\", [])\n",
    "\n",
    "#             # --- Process Text ---\n",
    "#             combined_text = (title + \" \" + abstract).strip()\n",
    "#             text_emb = generate_embedding(combined_text)\n",
    "\n",
    "#             if text_emb is not None:\n",
    "#                 text_embeddings_buffer.append(text_emb)\n",
    "#                 text_paper_ids.append(paper_id)\n",
    "#                 text_titles.append(title)\n",
    "\n",
    "#             # --- Process Equations ---\n",
    "#             for eq in equations:\n",
    "#                 if not eq.strip():\n",
    "#                     continue\n",
    "#                 eq_emb = generate_embedding(eq)\n",
    "#                 if eq_emb is not None:\n",
    "#                     eq_embeddings_buffer.append(eq_emb)\n",
    "#                     eq_paper_ids.append(paper_id)\n",
    "#                     eq_strings.append(eq)\n",
    "\n",
    "#             # --- When batch full ---\n",
    "#             if (idx + 1) % batch_size == 0:\n",
    "#                 if text_embeddings_buffer:\n",
    "#                     text_index.add(np.vstack(text_embeddings_buffer))\n",
    "#                     text_embeddings_buffer.clear()\n",
    "\n",
    "#                 if eq_embeddings_buffer:\n",
    "#                     eq_index.add(np.vstack(eq_embeddings_buffer))\n",
    "#                     eq_embeddings_buffer.clear()\n",
    "\n",
    "#                 gc.collect()\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ö†Ô∏è Error at paper {paper.get('id', '')}: {e}\")\n",
    "#             continue\n",
    "\n",
    "# # --- After last batch ---\n",
    "# if text_embeddings_buffer:\n",
    "#     text_index.add(np.vstack(text_embeddings_buffer))\n",
    "# if eq_embeddings_buffer:\n",
    "#     eq_index.add(np.vstack(eq_embeddings_buffer))\n",
    "# gc.collect()\n",
    "\n",
    "# # --- Save Outputs ---\n",
    "# faiss.write_index(text_index, output_text_faiss)\n",
    "# print(f\"‚úÖ Text FAISS index saved to: {output_text_faiss}\")\n",
    "\n",
    "# faiss.write_index(eq_index, output_eq_faiss)\n",
    "# print(f\"‚úÖ Equation FAISS index saved to: {output_eq_faiss}\")\n",
    "\n",
    "# pd.DataFrame({\n",
    "#     \"id\": text_paper_ids,\n",
    "#     \"title\": text_titles\n",
    "# }).to_csv(output_text_mapping, index=False)\n",
    "# print(f\"‚úÖ Text ID-Title mapping saved to: {output_text_mapping}\")\n",
    "\n",
    "# pd.DataFrame({\n",
    "#     \"id\": eq_paper_ids,\n",
    "#     \"equation\": eq_strings\n",
    "# }).to_csv(output_eq_mapping, index=False)\n",
    "# print(f\"‚úÖ Equation ID-Equation mapping saved to: {output_eq_mapping}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EMBEDDING GENERATION USING SCIBERT AND ADDITION TO FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:10:50.751943Z",
     "iopub.status.busy": "2025-05-20T16:10:50.751562Z",
     "iopub.status.idle": "2025-05-20T16:43:21.761341Z",
     "shell.execute_reply": "2025-05-20T16:43:21.760116Z",
     "shell.execute_reply.started": "2025-05-20T16:10:50.751916Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcc916f4c834cf09a633a5322873477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca6ab31f55a40f28a3514f42a3198b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:11:19.139677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747757479.505850      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747757479.605611      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8e61a5ffc440c8b1ef7c8c471d766d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1000 Papers: 6it [00:03,  1.76it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b0937d165543888dc68545cd079483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1000 Papers: 3000it [31:31,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text FAISS index saved to: /kaggle/working/text_faiss_index_1000.bin\n",
      "‚úÖ Equation FAISS index saved to: /kaggle/working/eq_faiss_index_1000.bin\n",
      "‚úÖ Text ID-Title mapping saved to: /kaggle/working/text_id_title_mapping_1000.csv\n",
      "‚úÖ Equation ID-Equation mapping saved to: /kaggle/working/eq_id_eq_mapping_1000.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gc\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# --- CONFIG ---\n",
    "input_path = \"/kaggle/working/extracted_equations.jsonl\"\n",
    "output_text_faiss = \"/kaggle/working/text_faiss_index_1000.bin\"\n",
    "output_eq_faiss = \"/kaggle/working/eq_faiss_index_1000.bin\"\n",
    "output_text_mapping = \"/kaggle/working/text_id_title_mapping_1000.csv\"\n",
    "output_eq_mapping = \"/kaggle/working/eq_id_eq_mapping_1000.csv\"\n",
    "\n",
    "num_papers_to_process = 3000  # <-- limit to 1000 only\n",
    "\n",
    "# --- Load SciBERT ---\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "device = torch.device(\"cpu\")  # <-- force CPU\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Helper ---\n",
    "def generate_embedding(text):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# --- Initialize FAISS ---\n",
    "embedding_dim = 768\n",
    "text_index = faiss.IndexFlatL2(embedding_dim)\n",
    "eq_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# --- Mappings ---\n",
    "text_paper_ids = []\n",
    "text_titles = []\n",
    "\n",
    "eq_paper_ids = []\n",
    "eq_strings = []\n",
    "\n",
    "# --- Embedding Buffers ---\n",
    "text_embeddings = []\n",
    "eq_embeddings = []\n",
    "\n",
    "# --- Start ---\n",
    "with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "    for idx, line in tqdm(enumerate(infile), desc=\"Processing 1000 Papers\"):\n",
    "        if idx >= num_papers_to_process:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "            paper_id = paper.get(\"id\", \"\")\n",
    "            title = paper.get(\"title\", \"\")\n",
    "            abstract = paper.get(\"abstract\", \"\")\n",
    "            equations = paper.get(\"equations\", [])\n",
    "\n",
    "            # Text Embedding\n",
    "            combined_text = (title + \" \" + abstract).strip()\n",
    "            text_emb = generate_embedding(combined_text)\n",
    "\n",
    "            if text_emb is not None:\n",
    "                text_embeddings.append(text_emb)\n",
    "                text_paper_ids.append(paper_id)\n",
    "                text_titles.append(title)\n",
    "\n",
    "            # Equation Embeddings\n",
    "            for eq in equations:\n",
    "                if not eq.strip():\n",
    "                    continue\n",
    "                eq_emb = generate_embedding(eq)\n",
    "                if eq_emb is not None:\n",
    "                    eq_embeddings.append(eq_emb)\n",
    "                    eq_paper_ids.append(paper_id)\n",
    "                    eq_strings.append(eq)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error at paper {paper.get('id', '')}: {e}\")\n",
    "            continue\n",
    "\n",
    "# --- Add to FAISS ---\n",
    "if text_embeddings:\n",
    "    text_index.add(np.vstack(text_embeddings))\n",
    "\n",
    "if eq_embeddings:\n",
    "    eq_index.add(np.vstack(eq_embeddings))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# --- Save Outputs ---\n",
    "faiss.write_index(text_index, output_text_faiss)\n",
    "print(f\"‚úÖ Text FAISS index saved to: {output_text_faiss}\")\n",
    "\n",
    "faiss.write_index(eq_index, output_eq_faiss)\n",
    "print(f\"‚úÖ Equation FAISS index saved to: {output_eq_faiss}\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"id\": text_paper_ids,\n",
    "    \"title\": text_titles\n",
    "}).to_csv(output_text_mapping, index=False)\n",
    "print(f\"‚úÖ Text ID-Title mapping saved to: {output_text_mapping}\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"id\": eq_paper_ids,\n",
    "    \"equation\": eq_strings\n",
    "}).to_csv(output_eq_mapping, index=False)\n",
    "print(f\"‚úÖ Equation ID-Equation mapping saved to: {output_eq_mapping}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEARCH USING ONLY SCIBERT AND FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:43:21.763957Z",
     "iopub.status.busy": "2025-05-20T16:43:21.763302Z",
     "iopub.status.idle": "2025-05-20T16:43:21.769773Z",
     "shell.execute_reply": "2025-05-20T16:43:21.768748Z",
     "shell.execute_reply.started": "2025-05-20T16:43:21.763930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "\n",
    "# # CONFIGS\n",
    "# text_faiss_path = \"/kaggle/working/text_faiss_index_1000.bin\"\n",
    "# eq_faiss_path = \"/kaggle/working/eq_faiss_index_1000.bin\"\n",
    "# text_mapping_path = \"/kaggle/working/text_id_title_mapping_1000.csv\"\n",
    "# eq_mapping_path = \"/kaggle/working/eq_id_eq_mapping_1000.csv\"\n",
    "\n",
    "# top_k = 5  # How many results to show\n",
    "\n",
    "# # Load SciBERT\n",
    "# model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "# device = torch.device(\"cpu\")  # Force CPU\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # Embedding Function\n",
    "# def generate_embedding(text):\n",
    "#     if not text.strip():\n",
    "#         return None\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# # Latex cleaner for equations\n",
    "# def clean_equation_to_latex(eq_text):\n",
    "#     # Basic: Add surrounding \"$\" if not already\n",
    "#     eq_text = eq_text.strip()\n",
    "#     if not (eq_text.startswith(\"$\") and eq_text.endswith(\"$\")):\n",
    "#         eq_text = f\"${eq_text}$\"\n",
    "#     return eq_text\n",
    "\n",
    "# # Load FAISS Indexes\n",
    "# text_index = faiss.read_index(text_faiss_path)\n",
    "# eq_index = faiss.read_index(eq_faiss_path)\n",
    "\n",
    "# # Load Mapping CSVs\n",
    "# text_mapping = pd.read_csv(text_mapping_path)\n",
    "# eq_mapping = pd.read_csv(eq_mapping_path)\n",
    "\n",
    "# # --- User Input ---\n",
    "# query_type = input(\"Enter query type (text / equation): \").strip().lower()\n",
    "# query = input(\"Enter your query: \").strip()\n",
    "\n",
    "# # --- Preprocess Query ---\n",
    "# if query_type == \"equation\":\n",
    "#     query = clean_equation_to_latex(query)  # Add LaTeX formatting\n",
    "\n",
    "# # --- Embed Query ---\n",
    "# query_emb = generate_embedding(query)\n",
    "# query_emb = np.expand_dims(query_emb, axis=0)  # FAISS expects 2D\n",
    "\n",
    "# # --- Search ---\n",
    "# if query_type == \"text\":\n",
    "#     distances, indices = text_index.search(query_emb, top_k)\n",
    "#     print(\"\\nüîé Top matches for your TEXT query:\")\n",
    "#     for dist, idx in zip(distances[0], indices[0]):\n",
    "#         paper_id = text_mapping.iloc[idx][\"id\"]\n",
    "#         title = text_mapping.iloc[idx][\"title\"]\n",
    "#         print(f\"Paper ID: {paper_id} | Title: {title} | Distance: {dist:.4f}\")\n",
    "\n",
    "# elif query_type == \"equation\":\n",
    "#     distances, indices = eq_index.search(query_emb, top_k)\n",
    "#     print(\"\\nüîé Top matches for your EQUATION query:\")\n",
    "#     for dist, idx in zip(distances[0], indices[0]):\n",
    "#         paper_id = eq_mapping.iloc[idx][\"id\"]\n",
    "#         equation = eq_mapping.iloc[idx][\"equation\"]\n",
    "#         print(f\"Paper ID: {paper_id} | Equation: {equation} | Distance: {dist:.4f}\")\n",
    "\n",
    "# else:\n",
    "#     print(\"‚ùå Invalid query type! Please type 'text' or 'equation'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:43:21.771284Z",
     "iopub.status.busy": "2025-05-20T16:43:21.770909Z",
     "iopub.status.idle": "2025-05-20T16:43:22.957058Z",
     "shell.execute_reply": "2025-05-20T16:43:22.955738Z",
     "shell.execute_reply.started": "2025-05-20T16:43:21.771243Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:43:22.958599Z",
     "iopub.status.busy": "2025-05-20T16:43:22.958248Z",
     "iopub.status.idle": "2025-05-20T16:43:28.209492Z",
     "shell.execute_reply": "2025-05-20T16:43:28.208204Z",
     "shell.execute_reply.started": "2025-05-20T16:43:22.958576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank_bm25) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank_bm25) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rank_bm25) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rank_bm25) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rank_bm25) (2024.2.0)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEARCH USING ONLY BM25**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:43:28.211624Z",
     "iopub.status.busy": "2025-05-20T16:43:28.211177Z",
     "iopub.status.idle": "2025-05-20T16:43:28.219549Z",
     "shell.execute_reply": "2025-05-20T16:43:28.218177Z",
     "shell.execute_reply.started": "2025-05-20T16:43:28.211582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# from rank_bm25 import BM25Okapi\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # --- CONFIG ---\n",
    "# input_jsonl = \"/kaggle/working/extracted_equations.jsonl\"\n",
    "# top_k = 5  # How many results to show\n",
    "# max_papers = 1000  # Limit to 1000 papers for faster processing\n",
    "\n",
    "# # --- Load Papers ---\n",
    "# papers = []\n",
    "# with open(input_jsonl, 'r', encoding='utf-8') as f:\n",
    "#     for idx, line in enumerate(f):\n",
    "#         if idx >= max_papers:\n",
    "#             break\n",
    "#         papers.append(json.loads(line))\n",
    "\n",
    "# # --- Preprocess ---\n",
    "# text_corpus = []\n",
    "# text_id_title = []\n",
    "\n",
    "# eq_corpus = []\n",
    "# eq_id_eq = []\n",
    "\n",
    "# for paper in papers:\n",
    "#     paper_id = paper.get(\"id\", \"\")\n",
    "#     title = paper.get(\"title\", \"\")\n",
    "#     abstract = paper.get(\"abstract\", \"\")\n",
    "#     equations = paper.get(\"equations\", [])\n",
    "\n",
    "#     # Text corpus: title + abstract\n",
    "#     combined_text = (title + \" \" + abstract).strip()\n",
    "#     tokens = word_tokenize(combined_text.lower())\n",
    "#     text_corpus.append(tokens)\n",
    "#     text_id_title.append((paper_id, title))\n",
    "\n",
    "#     # Equation corpus: each equation separately\n",
    "#     for eq in equations:\n",
    "#         if eq.strip():\n",
    "#             eq_tokens = word_tokenize(eq.lower())\n",
    "#             eq_corpus.append(eq_tokens)\n",
    "#             eq_id_eq.append((paper_id, eq))\n",
    "\n",
    "# # --- Build BM25 Indexes ---\n",
    "# text_bm25 = BM25Okapi(text_corpus)\n",
    "# eq_bm25 = BM25Okapi(eq_corpus)\n",
    "\n",
    "# # --- Query Time ---\n",
    "# query_type = input(\"Enter query type (text / equation): \").strip().lower()\n",
    "# query = input(\"Enter your query: \").strip()\n",
    "\n",
    "# query_tokens = word_tokenize(query.lower())\n",
    "\n",
    "# # --- Search ---\n",
    "# if query_type == \"text\":\n",
    "#     scores = text_bm25.get_scores(query_tokens)\n",
    "#     top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    \n",
    "#     print(\"\\nüîé Top matches for your TEXT query:\")\n",
    "#     for idx in top_indices:\n",
    "#         paper_id, title = text_id_title[idx]\n",
    "#         print(f\"Paper ID: {paper_id} | Title: {title} | BM25 Score: {scores[idx]:.4f}\")\n",
    "\n",
    "# elif query_type == \"equation\":\n",
    "#     scores = eq_bm25.get_scores(query_tokens)\n",
    "#     top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "\n",
    "#     print(\"\\nüîé Top matches for your EQUATION query:\")\n",
    "#     for idx in top_indices:\n",
    "#         paper_id, equation = eq_id_eq[idx]\n",
    "#         print(f\"Paper ID: {paper_id} | Equation: {equation} | BM25 Score: {scores[idx]:.4f}\")\n",
    "\n",
    "# else:\n",
    "#     print(\"‚ùå Invalid query type! Please type 'text' or 'equation'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HYBRID MODEL USING BOTH LEXICAL AND SEMANTIC SEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:43:28.221747Z",
     "iopub.status.busy": "2025-05-20T16:43:28.221325Z",
     "iopub.status.idle": "2025-05-20T16:48:23.634209Z",
     "shell.execute_reply": "2025-05-20T16:48:23.632808Z",
     "shell.execute_reply.started": "2025-05-20T16:43:28.221722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query type (text / equation):  planet\n",
      "Enter your query:  planet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Invalid query type! Please type 'text' or 'equation'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# --- CONFIG ---\n",
    "input_jsonl = \"/kaggle/working/extracted_equations.jsonl\"\n",
    "text_faiss_path = \"/kaggle/working/text_faiss_index_1000.bin\"\n",
    "eq_faiss_path = \"/kaggle/working/eq_faiss_index_1000.bin\"\n",
    "text_mapping_path = \"/kaggle/working/text_id_title_mapping_1000.csv\"\n",
    "eq_mapping_path = \"/kaggle/working/eq_id_eq_mapping_1000.csv\"\n",
    "\n",
    "max_papers = 200\n",
    "top_k_bm25 = 100\n",
    "top_k_final = 5\n",
    "\n",
    "# --- Load Papers ---\n",
    "papers = []\n",
    "with open(input_jsonl, 'r', encoding='utf-8') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx >= max_papers:\n",
    "            break\n",
    "        papers.append(json.loads(line))\n",
    "\n",
    "# --- Preprocess for BM25 ---\n",
    "text_corpus = []\n",
    "text_id_title = []\n",
    "\n",
    "for paper in papers:\n",
    "    paper_id = paper.get(\"id\", \"\")\n",
    "    title = paper.get(\"title\", \"\")\n",
    "    abstract = paper.get(\"abstract\", \"\")\n",
    "    combined_text = (title + \" \" + abstract).strip()\n",
    "    tokens = word_tokenize(combined_text.lower())\n",
    "    text_corpus.append(tokens)\n",
    "    text_id_title.append((paper_id, title))\n",
    "\n",
    "bm25_model = BM25Okapi(text_corpus)\n",
    "\n",
    "# --- Load FAISS Indexes ---\n",
    "text_index = faiss.read_index(text_faiss_path)\n",
    "eq_index = faiss.read_index(eq_faiss_path)\n",
    "\n",
    "# --- Load Mappings ---\n",
    "text_mapping = pd.read_csv(text_mapping_path)\n",
    "eq_mapping = pd.read_csv(eq_mapping_path)\n",
    "\n",
    "# --- Load SciBERT (for query embedding) ---\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "device = torch.device(\"cpu\")  # Force CPU\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_embedding(text):\n",
    "    if not text.strip():\n",
    "        return None\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "def clean_equation_to_latex(eq_text):\n",
    "    eq_text = eq_text.strip()\n",
    "    if not (eq_text.startswith(\"$\") and eq_text.endswith(\"$\")):\n",
    "        eq_text = f\"${eq_text}$\"\n",
    "    return eq_text\n",
    "\n",
    "# --- User Input ---\n",
    "query_type = input(\"Enter query type (text / equation): \").strip().lower()\n",
    "query = input(\"Enter your query: \").strip()\n",
    "\n",
    "if query_type == \"equation\":\n",
    "    query = clean_equation_to_latex(query)\n",
    "\n",
    "# --- Embed Query ---\n",
    "query_emb = generate_embedding(query)\n",
    "query_emb = np.expand_dims(query_emb, axis=0)\n",
    "\n",
    "# --- Hybrid Search ---\n",
    "if query_type == \"text\":\n",
    "    # BM25 phase\n",
    "    query_tokens = word_tokenize(query.lower())\n",
    "    bm25_scores = bm25_model.get_scores(query_tokens)\n",
    "    bm25_top_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k_bm25]\n",
    "\n",
    "    # FAISS phase: Build mini index of top BM25 results\n",
    "    selected_vectors = np.vstack([text_index.reconstruct(idx) for idx in bm25_top_indices])\n",
    "\n",
    "    mini_index = faiss.IndexFlatL2(768)\n",
    "    mini_index.add(selected_vectors)\n",
    "\n",
    "    distances, faiss_indices = mini_index.search(query_emb, top_k_final)\n",
    "\n",
    "    threshold = 0.6  # You can tune it\n",
    "\n",
    "    if np.all(distances[0] <= threshold):\n",
    "        print(\"\\nüîé Top matches for your TEXT query (BM25 + FAISS Hybrid):\")\n",
    "        for dist, idx in zip(distances[0], faiss_indices[0]):\n",
    "            real_idx = bm25_top_indices[idx]\n",
    "            paper_id, title = text_id_title[real_idx]\n",
    "            print(f\"Paper ID: {paper_id} | Title: {title} | Distance: {dist:.4f}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è FAISS matches were not strong. Falling back to pure BM25 matches:\")\n",
    "        for idx in bm25_top_indices[:top_k_final]:\n",
    "            paper_id, title = text_id_title[idx]\n",
    "            print(f\"Paper ID: {paper_id} | Title: {title}\")\n",
    "\n",
    "elif query_type == \"equation\":\n",
    "    # Direct FAISS on equation index\n",
    "    distances, indices = eq_index.search(query_emb, top_k_final)\n",
    "    print(\"\\nüîé Top matches for your EQUATION query (FAISS only):\")\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        paper_id = eq_mapping.iloc[idx][\"id\"]\n",
    "        equation = eq_mapping.iloc[idx][\"equation\"]\n",
    "        print(f\"Paper ID: {paper_id} | Equation: {equation} | Distance: {dist:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Invalid query type! Please type 'text' or 'equation'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:48:23.637720Z",
     "iopub.status.busy": "2025-05-20T16:48:23.637001Z",
     "iopub.status.idle": "2025-05-20T16:48:23.649516Z",
     "shell.execute_reply": "2025-05-20T16:48:23.647443Z",
     "shell.execute_reply.started": "2025-05-20T16:48:23.637687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import faiss\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from rank_bm25 import BM25Okapi\n",
    "# import torch\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # --- CONFIG ---\n",
    "# input_jsonl = \"/kaggle/working/extracted_equations.jsonl\"\n",
    "# text_faiss_path = \"/kaggle/working/text_faiss_index_1000.bin\"\n",
    "# eq_faiss_path = \"/kaggle/working/eq_faiss_index_1000.bin\"\n",
    "# text_mapping_path = \"/kaggle/working/text_id_title_mapping_1000.csv\"\n",
    "# eq_mapping_path = \"/kaggle/working/eq_id_eq_mapping_1000.csv\"\n",
    "\n",
    "# max_papers = 1000\n",
    "# top_k_bm25 = 50\n",
    "# top_k_final = 5\n",
    "\n",
    "# # --- Load Papers into Memory ---\n",
    "# def load_extracted_equations(file_path):\n",
    "#     \"\"\"\n",
    "#     Loads the extracted equations file into memory as a dictionary for fast lookups.\n",
    "#     \"\"\"\n",
    "#     data = {}\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             paper = json.loads(line)\n",
    "#             data[paper[\"id\"]] = paper  # Use paper ID as the key\n",
    "#     return data\n",
    "\n",
    "# extracted_data = load_extracted_equations(input_jsonl)\n",
    "\n",
    "# # --- Preprocess for BM25 ---\n",
    "# papers = list(extracted_data.values())[:max_papers]  # Limit number of papers to process\n",
    "# text_corpus = []\n",
    "# text_id_title = []\n",
    "\n",
    "# for paper in papers:\n",
    "#     paper_id = paper.get(\"id\", \"\")\n",
    "#     title = paper.get(\"title\", \"\")\n",
    "#     abstract = paper.get(\"abstract\", \"\")\n",
    "#     combined_text = (title + \" \" + abstract).strip()\n",
    "#     tokens = word_tokenize(combined_text.lower())\n",
    "#     text_corpus.append(tokens)\n",
    "#     text_id_title.append((paper_id, title))\n",
    "\n",
    "# bm25_model = BM25Okapi(text_corpus)\n",
    "\n",
    "# # --- Load FAISS Indexes ---\n",
    "# text_index = faiss.read_index(text_faiss_path)\n",
    "# eq_index = faiss.read_index(eq_faiss_path)\n",
    "\n",
    "# # --- Load Mappings ---\n",
    "# text_mapping = pd.read_csv(text_mapping_path)\n",
    "# eq_mapping = pd.read_csv(eq_mapping_path, dtype={\"id\": str})\n",
    "\n",
    "# # --- Load SciBERT (for query embedding) ---\n",
    "# model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "# device = torch.device(\"cpu\")  # Force CPU\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# def generate_embedding(text):\n",
    "#     if not text.strip():\n",
    "#         return None\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# def clean_equation_to_latex(eq_text):\n",
    "#     eq_text = eq_text.strip()\n",
    "#     if not (eq_text.startswith(\"$\") and eq_text.endswith(\"$\")):\n",
    "#         eq_text = f\"${eq_text}$\"\n",
    "#     return eq_text\n",
    "\n",
    "# # --- User Input ---\n",
    "# query_type = input(\"Enter query type (text / equation): \").strip().lower()\n",
    "# query = input(\"Enter your query: \").strip()\n",
    "\n",
    "# if query_type == \"equation\":\n",
    "#    query = clean_equation_to_latex(query)\n",
    "\n",
    "# # --- Embed Query ---\n",
    "# query_emb = generate_embedding(query)\n",
    "# query_emb = np.expand_dims(query_emb, axis=0)\n",
    "\n",
    "# # --- Hybrid Search ---\n",
    "# if query_type == \"text\":\n",
    "#     # BM25 phase\n",
    "#     query_tokens = word_tokenize(query.lower())\n",
    "#     bm25_scores = bm25_model.get_scores(query_tokens)\n",
    "#     bm25_top_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k_bm25]\n",
    "\n",
    "#     # FAISS phase: Build mini index of top BM25 results\n",
    "#     selected_vectors = np.vstack([text_index.reconstruct(idx) for idx in bm25_top_indices])\n",
    "\n",
    "#     mini_index = faiss.IndexFlatL2(768)\n",
    "#     mini_index.add(selected_vectors)\n",
    "\n",
    "#     distances, faiss_indices = mini_index.search(query_emb, top_k_final)\n",
    "\n",
    "#     threshold = 0.6  # You can tune it\n",
    "\n",
    "#     if np.all(distances[0] <= threshold):\n",
    "#         print(\"\\nüîé Top matches for your TEXT query (BM25 + FAISS Hybrid):\")\n",
    "#         for dist, idx in zip(distances[0], faiss_indices[0]):\n",
    "#             real_idx = bm25_top_indices[idx]\n",
    "#             paper_id = text_id_title[real_idx][0]\n",
    "\n",
    "#             # Fetch additional details from preloaded extracted_data\n",
    "#             paper_details = extracted_data.get(paper_id, {})\n",
    "#             url = paper_details.get(\"url\", \"Unavailable\")\n",
    "#             title = paper_details.get(\"title\", \"No Title\")\n",
    "#             authors = paper_details.get(\"authors\", \"Unknown\")\n",
    "#             abstract = paper_details.get(\"abstract\", \"No Abstract\")\n",
    "\n",
    "#             # Display results\n",
    "#             print(f\"Paper ID: {paper_id}\")\n",
    "#             print(f\"Title: {title}\")\n",
    "#             print(f\"Abstract: {abstract}\")\n",
    "#             print(f\"URL: {url}\")\n",
    "#             print(f\"Distance: {dist:.4f}\")\n",
    "#             print()\n",
    "\n",
    "#     else:\n",
    "#         print(\"\\n‚ö†Ô∏è FAISS matches were not strong. Falling back to pure BM25 matches:\")\n",
    "#         for idx in bm25_top_indices[:top_k_final]:\n",
    "#             paper_id = text_id_title[idx][0]\n",
    "\n",
    "#             # Fetch additional details from preloaded extracted_data\n",
    "#             paper_details = extracted_data.get(paper_id, {})\n",
    "#             url = paper_details.get(\"url\", \"Unavailable\")\n",
    "#             title = paper_details.get(\"title\", \"No Title\")\n",
    "#             authors = paper_details.get(\"authors\", \"Unknown\")\n",
    "#             abstract = paper_details.get(\"abstract\", \"No Abstract\")\n",
    "\n",
    "#             # Display results\n",
    "#             print(f\"Paper ID: {paper_id}\")\n",
    "#             print(f\"Title: {title}\")\n",
    "#             print(f\"Abstract: {abstract}\")\n",
    "#             print(f\"URL: {url}\")\n",
    "#             print()\n",
    "# elif query_type == \"equation\":\n",
    "#     distances, indices = eq_index.search(query_emb, top_k_final)\n",
    "#     print(\"\\nüîé Top matches for your EQUATION query (FAISS only):\")\n",
    "\n",
    "#     for dist, idx in zip(distances[0], indices[0]):\n",
    "#         # Get paper ID from mapping\n",
    "#         raw_paper_id = str(eq_mapping.iloc[idx][\"id\"]).strip()  # <-- Fix here\n",
    "\n",
    "#         # Correct padding if needed\n",
    "#         if len(raw_paper_id.split(\".\")[0]) == 3:  # e.g., \"705.0931\"\n",
    "#             paper_id = \"0\" + raw_paper_id         # make it \"0705.0931\"\n",
    "#         else:\n",
    "#             paper_id = raw_paper_id               # already correct\n",
    "\n",
    "#         # Lookup full extracted paper\n",
    "#         paper_details = extracted_data.get(paper_id, {})\n",
    "#         title = paper_details.get(\"title\", \"No Title\")\n",
    "#         authors = paper_details.get(\"authors\", \"Unknown\")\n",
    "#         abstract = paper_details.get(\"abstract\", \"No Abstract\")\n",
    "#         url = paper_details.get(\"url\", \"Unavailable\")\n",
    "\n",
    "#         print(f\"Paper ID: {paper_id}\")\n",
    "#         print(f\"Title: {title}\")\n",
    "#         print(f\"Authors: {authors}\")\n",
    "#         print(f\"Abstract: {abstract}\")\n",
    "#         print(f\"URL: {url}\")\n",
    "#         print(f\"Distance: {dist:.4f}\")\n",
    "#         print()\n",
    "\n",
    "# else:\n",
    "#     print(\"‚ùå Invalid query type! Please type 'text' or 'equation'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-05-20T16:48:23.651621Z",
     "iopub.status.busy": "2025-05-20T16:48:23.651251Z",
     "iopub.status.idle": "2025-05-20T16:48:36.823277Z",
     "shell.execute_reply": "2025-05-20T16:48:36.821822Z",
     "shell.execute_reply.started": "2025-05-20T16:48:23.651585Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
      "Downloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-7.2.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.1)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.26.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\n",
      "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
      "Successfully installed pydeck-0.9.1 streamlit-1.45.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyngrok\n",
    "!ngrok authtoken Authtokenillihaku\n",
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-05-20T16:48:36.826289Z",
     "iopub.status.busy": "2025-05-20T16:48:36.825842Z",
     "iopub.status.idle": "2025-05-20T16:48:36.838254Z",
     "shell.execute_reply": "2025-05-20T16:48:36.836762Z",
     "shell.execute_reply.started": "2025-05-20T16:48:36.826241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%writefile app.py\n",
    "# import streamlit as st\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import faiss\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import pandas as pd\n",
    "# from rank_bm25 import BM25Okapi\n",
    "\n",
    "# # --- CONFIG ---\n",
    "# input_jsonl = \"/kaggle/working/extracted_equations.jsonl\"\n",
    "# text_faiss_path = \"/kaggle/working/text_faiss_index_1000.bin\"\n",
    "# eq_faiss_path = \"/kaggle/working/eq_faiss_index_1000.bin\"\n",
    "# text_mapping_path = \"//kaggle/working/text_id_title_mapping_1000.csv\"\n",
    "# eq_mapping_path = \"/kaggle/working/eq_id_eq_mapping_1000.csv\"\n",
    "\n",
    "# max_papers = 1000\n",
    "# top_k_bm25 = 10\n",
    "# top_k_final = 5\n",
    "\n",
    "# # --- Load Extracted Data ---\n",
    "# def load_extracted_equations(file_path):\n",
    "#     data = {}\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             paper = json.loads(line)\n",
    "#             data[paper[\"id\"]] = paper\n",
    "#     return data\n",
    "\n",
    "# extracted_data = load_extracted_equations(input_jsonl)\n",
    "\n",
    "# # --- Preprocess for BM25 ---\n",
    "# papers = list(extracted_data.values())[:max_papers]  # Limit number of papers to process\n",
    "# text_corpus = []\n",
    "# text_id_title = []\n",
    "\n",
    "# for paper in papers:\n",
    "#     paper_id = paper.get(\"id\", \"\")\n",
    "#     title = paper.get(\"title\", \"\")\n",
    "#     abstract = paper.get(\"abstract\", \"\")\n",
    "#     combined_text = (title + \" \" + abstract).strip()\n",
    "#     tokens = word_tokenize(combined_text.lower())\n",
    "#     text_corpus.append(tokens)\n",
    "#     text_id_title.append((paper_id, title))\n",
    "\n",
    "# bm25_model = BM25Okapi(text_corpus)\n",
    "\n",
    "\n",
    "# # --- Load FAISS Indexes ---\n",
    "# text_index = faiss.read_index(text_faiss_path)\n",
    "# eq_index = faiss.read_index(eq_faiss_path)\n",
    "# # --- Load Mappings ---\n",
    "# text_mapping = pd.read_csv(text_mapping_path)\n",
    "# eq_mapping = pd.read_csv(eq_mapping_path, dtype={\"id\": str})\n",
    "\n",
    "# # --- Load SciBERT Model ---\n",
    "# model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "# device = torch.device(\"cpu\")\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# def generate_embedding(text):\n",
    "#     \"\"\"Generate embeddings for the query.\"\"\"\n",
    "#     if not text.strip():\n",
    "#         return None\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# def clean_equation_to_latex(eq_text):\n",
    "#     eq_text = eq_text.strip()\n",
    "#     if not (eq_text.startswith(\"$\") and eq_text.endswith(\"$\")):\n",
    "#         eq_text = f\"${eq_text}$\"\n",
    "#     return eq_text\n",
    "\n",
    "# # --- Streamlit UI ---\n",
    "# st.title(\"Hybrid Search Engine\")\n",
    "# st.sidebar.header(\"Query Options\")\n",
    "# st.sidebar.info(\"**Note:** You can enter either a text query or an equation query, but not both!\")\n",
    "\n",
    "# # Input fields\n",
    "# query_type = None\n",
    "# text_query = st.text_input(\"Text Query\")\n",
    "# equation_query = st.text_input(\"Equation Query\")\n",
    "\n",
    "# if text_query.strip() and equation_query.strip():\n",
    "#     st.error(\"‚ö†Ô∏è Please use only one input box. Clear one before submitting.\")\n",
    "\n",
    "# elif text_query.strip():\n",
    "#     query_type = \"text\"\n",
    "#     query = text_query.strip()\n",
    "# elif equation_query.strip():\n",
    "#     query_type = \"equation\"\n",
    "#     query = clean_equation_to_latex(equation_query)  # Ensure LaTeX formatting for equations\n",
    "# else:\n",
    "#     st.info(\"Awaiting your input...\")\n",
    "# # --- Embed Query ---\n",
    "# query_emb = generate_embedding(query)\n",
    "# query_emb = np.expand_dims(query_emb, axis=0)\n",
    "\n",
    "# # Submit button\n",
    "# if st.button(\"Search\"):\n",
    "#     if query_type == \"text\":\n",
    "#         # --- Text Query ---\n",
    "#         query_tokens = word_tokenize(query.lower())\n",
    "#         bm25_scores = bm25_model.get_scores(query_tokens)\n",
    "#         bm25_top_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k_bm25]\n",
    "\n",
    "#         # FAISS phase: Build mini index of top BM25 results\n",
    "#         selected_vectors = np.vstack([text_index.reconstruct(idx) for idx in bm25_top_indices])\n",
    "\n",
    "#         mini_index = faiss.IndexFlatL2(768)\n",
    "#         mini_index.add(selected_vectors)\n",
    "\n",
    "#         distances, faiss_indices = mini_index.search(query_emb, top_k_final)\n",
    "\n",
    "#         threshold = 0.6\n",
    "#         if np.all(distances[0] <= threshold):\n",
    "#           st.success(\"\\nüîéTop matches for your TEXT query (BM25 + FAISS Hybrid):\")\n",
    "#           for dist, idx in zip(distances[0], faiss_indices[0]):\n",
    "#             real_idx = bm25_top_indices[idx]\n",
    "#             paper_id = text_id_title[real_idx][0]\n",
    "\n",
    "#             # Fetch additional details from preloaded extracted_data\n",
    "#             paper_details = extracted_data.get(paper_id, {})\n",
    "#             url = paper_details.get(\"url\", \"Unavailable\")\n",
    "#             title = paper_details.get(\"title\", \"No Title\")\n",
    "#             authors = paper_details.get(\"authors\", \"Unknown\")\n",
    "#             abstract = paper_details.get(\"abstract\", \"No Abstract\")\n",
    "\n",
    "            \n",
    "#             st.write(f\"**Paper ID**: {paper_id}\")\n",
    "#             st.write(f\"**Title**: {paper_details.get('title', 'No Title')}\")\n",
    "#             st.write(f\"**Abstract**: {paper_details.get('abstract', 'No Abstract')}\")\n",
    "#             st.write(f\"[Link to Paper]({paper_details.get('url', '#')})\")\n",
    "#             st.write(\"---\")\n",
    "#         else:\n",
    "#           st.success(\"\\n‚ö†Ô∏èFAISS matches were not strong. Falling back to pure BM25 matches:\")\n",
    "#           for idx in bm25_top_indices[:top_k_final]:\n",
    "#              paper_id = text_id_title[idx][0]\n",
    "\n",
    "#              # Fetch additional details from preloaded extracted_data\n",
    "#              paper_details = extracted_data.get(paper_id, {})\n",
    "#              url = paper_details.get(\"url\", \"Unavailable\")\n",
    "#              title = paper_details.get(\"title\", \"No Title\")\n",
    "#              authors = paper_details.get(\"authors\", \"Unknown\")\n",
    "#              abstract = paper_details.get(\"abstract\", \"No Abstract\")\n",
    "\n",
    "             \n",
    "#              st.write(f\"**Paper ID**: {paper_id}\")\n",
    "#              st.write(f\"**Title**: {paper_details.get('title', 'No Title')}\")\n",
    "#              st.write(f\"**Abstract**: {paper_details.get('abstract', 'No Abstract')}\")\n",
    "#              st.write(f\"[Link to Paper]({paper_details.get('url', '#')})\")\n",
    "#              st.write(\"---\")\n",
    "\n",
    "#     elif query_type == \"equation\":\n",
    "#         # --- Equation Query ---\n",
    "#         distances, indices = eq_index.search(query_emb, top_k_final)\n",
    "#         st.success(\"\\nüîéTop matches for your EQUATION query (FAISS only):\")\n",
    "\n",
    "#         for dist, idx in zip(distances[0], indices[0]):\n",
    "#             # Get paper ID from mapping\n",
    "#             raw_paper_id = str(eq_mapping.iloc[idx][\"id\"]).strip()\n",
    "#             # Correct padding if needed\n",
    "#             if len(raw_paper_id.split(\".\")[0]) == 3:  # e.g., \"705.0931\"\n",
    "#               paper_id = \"0\" + raw_paper_id         # make it \"0705.0931\"\n",
    "#             else:\n",
    "#               paper_id = raw_paper_id\n",
    "\n",
    "#             paper_details = extracted_data.get(paper_id, {})\n",
    "#             url = paper_details.get(\"url\", \"Unavailable\")\n",
    "#             title = paper_details.get(\"title\", \"No Title\")\n",
    "#             authors = paper_details.get(\"authors\", \"Unknown\")\n",
    "#             abstract = paper_details.get(\"abstract\", \"No Abstract\")\n",
    "\n",
    "#             st.write(f\"**Paper ID**: {paper_id}\")\n",
    "#             st.write(f\"**Title**: {paper_details.get('title', 'No Title')}\")\n",
    "#             st.write(f\"**Abstract**: {paper_details.get('abstract', 'No Abstract')}\")\n",
    "#             st.write(f\"[Link to Paper]({paper_details.get('url', '#')})\")\n",
    "#             st.write(\"---\")\n",
    "#     else:\n",
    "#         st.error(\"‚ö†Ô∏è Please provide a valid query.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINAL STREAMLIT CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:48:36.840540Z",
     "iopub.status.busy": "2025-05-20T16:48:36.840155Z",
     "iopub.status.idle": "2025-05-20T16:48:36.870991Z",
     "shell.execute_reply": "2025-05-20T16:48:36.869874Z",
     "shell.execute_reply.started": "2025-05-20T16:48:36.840514Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_JSONL          = \"/kaggle/working/extracted_equations.jsonl\"\n",
    "TEXT_FAISS_PATH      = \"/kaggle/working/text_faiss_index_1000.bin\"\n",
    "EQ_FAISS_PATH        = \"/kaggle/working/eq_faiss_index_1000.bin\"\n",
    "TEXT_MAPPING_CSV     = \"/kaggle/working/text_id_title_mapping_1000.csv\"\n",
    "EQ_MAPPING_CSV       = \"/kaggle/working/eq_id_eq_mapping_1000.csv\"\n",
    "\n",
    "MAX_PAPERS           = 1000\n",
    "TOP_K_BM25           = 10\n",
    "TOP_K_FINAL          = 5\n",
    "FAISS_DIM            = 768\n",
    "TEXT_MODEL_NAME      = \"allenai/scibert_scivocab_uncased\"\n",
    "\n",
    "# --- Load extracted_data into dict for fast lookup ---\n",
    "@st.cache_data\n",
    "def load_extracted_data(path):\n",
    "    d = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            d[p[\"id\"]] = p\n",
    "    return d\n",
    "\n",
    "extracted_data = load_extracted_data(INPUT_JSONL)\n",
    "\n",
    "# --- Build BM25 corpora ---\n",
    "@st.cache_data\n",
    "def build_bm25():\n",
    "    papers = list(extracted_data.values())[:MAX_PAPERS]\n",
    "    corpus, id_title = [], []\n",
    "    for p in papers:\n",
    "        pid = p[\"id\"]\n",
    "        txt = (p.get(\"title\",\"\") + \" \" + p.get(\"abstract\",\"\")).strip()\n",
    "        tokens = word_tokenize(txt.lower())\n",
    "        corpus.append(tokens)\n",
    "        id_title.append((pid, p.get(\"title\",\"No Title\")))\n",
    "    return BM25Okapi(corpus), id_title\n",
    "\n",
    "bm25_model, text_id_title = build_bm25()\n",
    "\n",
    "# --- Load FAISS indexes & mappings ---\n",
    "@st.cache_resource\n",
    "def load_faiss_and_maps():\n",
    "    t_idx = faiss.read_index(TEXT_FAISS_PATH)\n",
    "    e_idx = faiss.read_index(EQ_FAISS_PATH)\n",
    "    t_map = pd.read_csv(TEXT_MAPPING_CSV, dtype={\"id\": str})\n",
    "    e_map = pd.read_csv(EQ_MAPPING_CSV, dtype={\"id\": str})\n",
    "    return t_idx, e_idx, t_map, e_map\n",
    "\n",
    "text_index, eq_index, text_map, eq_map = load_faiss_and_maps()\n",
    "\n",
    "# --- Load SciBERT ---\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_scibert():\n",
    "    tok = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "    mdl = AutoModel.from_pretrained(TEXT_MODEL_NAME).to(\"cpu\").eval()\n",
    "    return tok, mdl\n",
    "\n",
    "tokenizer, model = load_scibert()\n",
    "\n",
    "def embed(text: str):\n",
    "    if not text:\n",
    "        return None\n",
    "    inps = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inps)\n",
    "    return out.last_hidden_state.mean(1).squeeze().cpu().numpy()\n",
    "\n",
    "def to_latex(eq: str):\n",
    "    eq = eq.strip()\n",
    "    if not (eq.startswith(\"$\") and eq.endswith(\"$\")):\n",
    "        eq = f\"${eq}$\"\n",
    "    return eq\n",
    "\n",
    "# --- UI ---\n",
    "st.title(\"üîç Hybrid Search Engine\")\n",
    "st.sidebar.header(\"Enter *either* a Text Query *or* an Equation Query\")\n",
    "\n",
    "text_query      = st.sidebar.text_input(\"Text Query\")\n",
    "equation_query  = st.sidebar.text_input(\"Equation Query\")\n",
    "\n",
    "if st.sidebar.button(\"Search\"):\n",
    "    # Enforce single input\n",
    "    if text_query and equation_query:\n",
    "        st.sidebar.error(\"‚ùóÔ∏è Please clear one box before searching.\")\n",
    "        st.stop()\n",
    "    if not text_query and not equation_query:\n",
    "        st.sidebar.info(\"Awaiting input‚Ä¶\")\n",
    "        st.stop()\n",
    "\n",
    "    # Decide mode & prepare query\n",
    "    if text_query:\n",
    "        mode, q = \"text\", text_query.strip()\n",
    "    else:\n",
    "        mode, q = \"equation\", to_latex(equation_query.strip())\n",
    "\n",
    "    emb = embed(q)\n",
    "    if emb is None:\n",
    "        st.error(\"Could not embed your input.\")\n",
    "        st.stop()\n",
    "    emb = np.expand_dims(emb, 0)\n",
    "\n",
    "    if mode == \"text\":\n",
    "        # 1) BM25 top candidates\n",
    "        toks   = word_tokenize(q.lower())\n",
    "        scores = bm25_model.get_scores(toks)\n",
    "        top_bm = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:TOP_K_BM25]\n",
    "\n",
    "        # 2) FAISS re-ranking\n",
    "        small_idx = faiss.IndexFlatL2(FAISS_DIM)\n",
    "        mats = np.vstack([ text_index.reconstruct(i) for i in top_bm ])\n",
    "        small_idx.add(mats)\n",
    "        dists, idxs = small_idx.search(emb, TOP_K_FINAL)\n",
    "\n",
    "        st.header(\"Top Results (Hybrid BM25 + FAISS)\")\n",
    "        for dist, i in zip(dists[0], idxs[0]):\n",
    "            real = top_bm[i]\n",
    "            pid, title = text_id_title[real]\n",
    "            meta = extracted_data.get(pid, {})\n",
    "            st.subheader(f\"{title}  (ID: {pid})\")\n",
    "            st.write(meta.get(\"abstract\",\"\"))\n",
    "            st.markdown(f\"[View Paper]({meta.get('url','#')})  ‚Äî  Distance: **{dist:.3f}**\")\n",
    "            st.write(\"---\")\n",
    "\n",
    "    else:  # equation\n",
    "        dists, idxs = eq_index.search(emb, TOP_K_FINAL)\n",
    "        st.header(\"Top Results (Equation FAISS)\")\n",
    "        for dist, i in zip(dists[0], idxs[0]):\n",
    "            pid = eq_map.iloc[i][\"id\"]\n",
    "            # normalize 3-digit IDs\n",
    "            if len(pid.split(\".\")[0])==3: pid = \"0\"+pid\n",
    "            meta = extracted_data.get(pid, {})\n",
    "            st.subheader(f\"{meta.get('title','No Title')}  (ID: {pid})\")\n",
    "            st.write(meta.get(\"abstract\",\"\"))\n",
    "            st.markdown(f\"[View Paper]({meta.get('url','#')})  ‚Äî  Distance: **{dist:.3f}**\")\n",
    "            st.write(\"---\")\n",
    "else:\n",
    "    st.info(\"Enter a query and press **Search**.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:48:36.872737Z",
     "iopub.status.busy": "2025-05-20T16:48:36.872316Z",
     "iopub.status.idle": "2025-05-20T16:48:46.020704Z",
     "shell.execute_reply": "2025-05-20T16:48:46.018926Z",
     "shell.execute_reply.started": "2025-05-20T16:48:36.872702Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K\n",
      "added 44 packages in 8s\n",
      "\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K\n",
      "\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K9 packages are looking for funding\n",
      "\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K  run `npm fund` for details\n",
      "\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m10.8.2\u001b[39m -> \u001b[34m11.4.0\u001b[39m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m Changelog: \u001b[34mhttps://github.com/npm/cli/releases/tag/v11.4.0\u001b[39m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m To update run: \u001b[4mnpm install -g npm@11.4.0\u001b[24m\n",
      "\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n",
      "\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K"
     ]
    }
   ],
   "source": [
    "!npm install -g ngrok@5.0.0-beta.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T16:48:46.022869Z",
     "iopub.status.busy": "2025-05-20T16:48:46.022273Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K\u001b7\u001b[?47h\u001b[?1h\u001b=\u0002\u0007\u001b[H\u001b[2J\u001b[m\u001b[38;5;6m\u001b[48;5;16m\u001b[1m\u001b[1;1Hngrok\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                                           \u001b[m\u001b[38;5;7m\u001b[48;5;16m(Ctrl+C to quit)\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[2;1H                                                                                \u001b[m\u001b[38;5;6m\u001b[48;5;16m\u001b[3;1HSession Status                connecting\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                        \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[4;1HVersion                       3.22.1\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                            \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[5;1HWeb Interface                 http://127.0.0.1:4040\u001b[m\u001b[38;5;16m\u001b[48;5;16m                             \u001b[6;1H                                                                                \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[7;1HConnections                   ttl     opn     rt1     rt5     p50     p90     \u001b[m\u001b[38;5;16m\u001b[48;5;16m  \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[8;1H                              0       0       0.00    0.00    0.00    0.00    \u001b[m\u001b[38;5;16m\u001b[48;5;16m  \u001b[9;1H                                                                                \u001b[10;1H                                                                                \u001b[11;1H                                                                                \u001b[12;1H                                                                                \u001b[13;1H                                                                                \u001b[14;1H                                                                                \u001b[15;1H                                                                                \u001b[16;1H                                                                                \u001b[17;1H                                                                                \u001b[18;1H                                                                                \u001b[19;1H                                                                                \u001b[20;1H                                                                                \u001b[21;1H                                                                                \u001b[22;1H                                                                                \u001b[23;1H                                                                                \u001b[24;1H                                                                                \n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[m\u001b[38;5;5m\u001b[48;5;16m\u001b[3;1Hü§ñ\u001b[3;3H Want to hang with ngrokkers on our new Discord? http://ngrok.com/discord\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[4;1H                                    \u001b[m\u001b[38;5;2m\u001b[48;5;16m\u001b[5;1HSession Status                online\u001b[m\u001b[38;5;16m\u001b[48;5;16m               \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[6;1HAccount                       neuralsearchdrive1@gmail.com (Plan: Free)\u001b[7;1HVersion    \u001b[7;31H3.22.1\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                          \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[8;1HRegion\u001b[8;31HUnited\u001b[8;38HStates\u001b[8;45H(us)\u001b[m\u001b[38;5;16m\u001b[48;5;16m                              \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[9;1HWeb Interface                 http://127.0.0.1:4040\u001b[10;1HForwarding                    https://4e6f-34-13-204-238.ngrok-free.app -> http:\u001b[12;1HConnections                   ttl     opn     rt1     rt5     p50     p90     \u001b[13;1H                              0       0       0.00    0.00    0.00    0.00    \u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.13.204.238:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[9;1HLatency      \u001b[9;31H101ms\u001b[m\u001b[38;5;16m\u001b[48;5;16m                \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[10;1HWeb Interface\u001b[10;35H:/\u001b[10;38H127.0.0.1:404\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[10;52H                             \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[11;1HForwarding                    https://4e6f-34-13-204-238.ngrok-free.app -> http:\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[12;1H                                                                              \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[13;1HConnections\u001b[13;31Httl\u001b[13;39Hopn\u001b[13;47Hrt1 \u001b[13;55Hrt5 \u001b[13;63Hp5\u001b[13;66H \u001b[13;71Hp9\u001b[13;74H \u001b[14;1H                              0       0       0.00    0.00    0.00    0.00    \u001b[9;33H0\u001b[9;33H2\u001b[9;33H4\u001b[9;33H3\u001b[9;33H4\u001b[9;33H2\u001b[9;31H99ms\u001b[m\u001b[38;5;16m\u001b[48;5;16m \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[9;31H100ms\u001b[9;33H2\u001b[9;33H0\u001b[9;31H99ms\u001b[m\u001b[38;5;16m\u001b[48;5;16m \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[9;31H100ms"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py --server.port 8501 & npx ngrok http 8501"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 612177,
     "sourceId": 11852525,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
